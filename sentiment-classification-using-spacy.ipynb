{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDnsGjKidRQp"
   },
   "source": [
    "### Importing Necessary Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:04:50.184071Z",
     "iopub.status.busy": "2023-04-08T09:04:50.183537Z",
     "iopub.status.idle": "2023-04-08T09:04:51.281550Z",
     "shell.execute_reply": "2023-04-08T09:04:51.280350Z",
     "shell.execute_reply.started": "2023-04-08T09:04:50.184011Z"
    },
    "id": "WPuiBRl7aHiC",
    "outputId": "8883b945-f874-4700-e8c8-d5d6fffba8c4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SpaCy's small english model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more details regarding SpaCy models check here : https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:49.486041Z",
     "iopub.status.busy": "2023-04-08T09:02:49.485588Z",
     "iopub.status.idle": "2023-04-08T09:02:50.152975Z",
     "shell.execute_reply": "2023-04-08T09:02:50.151840Z",
     "shell.execute_reply.started": "2023-04-08T09:02:49.485991Z"
    },
    "id": "nujppuMjbTOB"
   },
   "outputs": [],
   "source": [
    "# Loading Spacy small model as nlp\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering all the Stop words which does not convey much meaning in the Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.154670Z",
     "iopub.status.busy": "2023-04-08T09:02:50.154362Z",
     "iopub.status.idle": "2023-04-08T09:02:50.160086Z",
     "shell.execute_reply": "2023-04-08T09:02:50.159123Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.154640Z"
    },
    "id": "m90EIAg5cTQG",
    "outputId": "c570dbff-1b65-4792-a2b0-342b24ba921d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "# Gathering all the stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.161805Z",
     "iopub.status.busy": "2023-04-08T09:02:50.161453Z",
     "iopub.status.idle": "2023-04-08T09:02:50.182429Z",
     "shell.execute_reply": "2023-04-08T09:02:50.181467Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.161773Z"
    },
    "id": "SmsVXflwcrrJ",
    "outputId": "33ad0c05-4ae0-4c49-9be1-a6ab1adaa812"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                           Sentence\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading yelp dataset\n",
    "data_yelp = pd.read_csv('dataset/all-data.csv',delimiter=',', encoding='latin-1', header=None)\n",
    "data_yelp = data_yelp.rename(columns=lambda x: ['Sentiment', 'Sentence'][x])\n",
    "data_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp['Sentiment'] = data_yelp['Sentiment'].map({'neutral':0,'positive':1,'negative':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_yelp_small['Sentiment'] = [1,-1,-1,0,0,1,1,-1,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_yelp_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_yelp[['Sentence','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  According to Gran , the company has no plans t...          0\n",
       "1  Technopolis plans to develop in stages an area...          0\n",
       "2  The international electronic industry company ...         -1\n",
       "3  With the new production plant the company woul...          1\n",
       "4  According to the company 's updated strategy f...          1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.185774Z",
     "iopub.status.busy": "2023-04-08T09:02:50.185438Z",
     "iopub.status.idle": "2023-04-08T09:02:50.196632Z",
     "shell.execute_reply": "2023-04-08T09:02:50.195535Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.185743Z"
    },
    "id": "EY9NeiFAdD0v",
    "outputId": "cc1a9800-fc37-4fa3-d5ae-149244b8d427"
   },
   "outputs": [],
   "source": [
    "# # Adding column names to the dataframe\n",
    "# columnName = ['Review','Sentiment']\n",
    "# data_yelp.columns = columnName\n",
    "# data_yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So here we can deduce that Sentiment 1 is Positive and 0 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.200190Z",
     "iopub.status.busy": "2023-04-08T09:02:50.199787Z",
     "iopub.status.idle": "2023-04-08T09:02:50.205650Z",
     "shell.execute_reply": "2023-04-08T09:02:50.204812Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.200149Z"
    },
    "id": "aUdmn-6AeckX",
    "outputId": "a93d9cc1-29ac-4e33-8b68-189e649a320c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data_yelp_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.207880Z",
     "iopub.status.busy": "2023-04-08T09:02:50.207472Z",
     "iopub.status.idle": "2023-04-08T09:02:50.225655Z",
     "shell.execute_reply": "2023-04-08T09:02:50.224594Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.207847Z"
    },
    "id": "qQueov_XdL7b",
    "outputId": "ddcabed8-a2c6-4dbd-dc39-b55808a1c95b"
   },
   "outputs": [],
   "source": [
    "# # Adding Amazon dataset and adding its column name\n",
    "# data_amz = pd.read_csv(\"dataset/amazon_cells_labelled.txt\",\n",
    "#                         sep='\\t', header= None)\n",
    "# data_amz.columns = columnName\n",
    "# data_amz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.227463Z",
     "iopub.status.busy": "2023-04-08T09:02:50.227067Z",
     "iopub.status.idle": "2023-04-08T09:02:50.235567Z",
     "shell.execute_reply": "2023-04-08T09:02:50.233458Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.227422Z"
    },
    "id": "1nz_dIL6efAL",
    "outputId": "ebdd55fb-19f3-4fd7-c83e-ae4a111b55df"
   },
   "outputs": [],
   "source": [
    "# print(data_amz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.237048Z",
     "iopub.status.busy": "2023-04-08T09:02:50.236720Z",
     "iopub.status.idle": "2023-04-08T09:02:50.252316Z",
     "shell.execute_reply": "2023-04-08T09:02:50.251328Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.237019Z"
    },
    "id": "RlmibXduefiM",
    "outputId": "54942c4b-b3b9-49d2-8ed4-96d9e36ef9db"
   },
   "outputs": [],
   "source": [
    "# # Adding IMdB dataset and adding its column name\n",
    "# data_imdb = pd.read_csv(\"dataset/imdb_labelled.txt\",\n",
    "#                         sep='\\t', header= None)\n",
    "# data_imdb.columns = columnName\n",
    "# data_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.253739Z",
     "iopub.status.busy": "2023-04-08T09:02:50.253409Z",
     "iopub.status.idle": "2023-04-08T09:02:50.258485Z",
     "shell.execute_reply": "2023-04-08T09:02:50.257366Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.253700Z"
    },
    "id": "W-KvuCuHeuUg",
    "outputId": "c65c1fd1-f36f-4064-be72-ee7cfeb16190"
   },
   "outputs": [],
   "source": [
    "# print(data_imdb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending all the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.260024Z",
     "iopub.status.busy": "2023-04-08T09:02:50.259676Z",
     "iopub.status.idle": "2023-04-08T09:02:50.270713Z",
     "shell.execute_reply": "2023-04-08T09:02:50.269794Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.259994Z"
    },
    "id": "e_XOzJ3Key4E",
    "outputId": "665e1e7c-eb23-4d3d-8c60-15950fc02ba8"
   },
   "outputs": [],
   "source": [
    "# # Merging all the three dataframes\n",
    "# data = data_yelp.append([data_amz, data_imdb], ignore_index=True)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.272457Z",
     "iopub.status.busy": "2023-04-08T09:02:50.272132Z",
     "iopub.status.idle": "2023-04-08T09:02:50.283972Z",
     "shell.execute_reply": "2023-04-08T09:02:50.282848Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.272427Z"
    },
    "id": "JHl1q35JfCmQ",
    "outputId": "089cebc8-75d6-45b8-9258-b2942e53a4df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    2879\n",
       " 1    1363\n",
       "-1     604\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment ditribution in the dataset\n",
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.285914Z",
     "iopub.status.busy": "2023-04-08T09:02:50.285393Z",
     "iopub.status.idle": "2023-04-08T09:02:50.293989Z",
     "shell.execute_reply": "2023-04-08T09:02:50.293199Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.285860Z"
    },
    "id": "n9L8Vq3jfHrl",
    "outputId": "92f815a0-77b6-4836-f9cc-239892cc7641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence     0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting information regarding the null entries in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.295712Z",
     "iopub.status.busy": "2023-04-08T09:02:50.295205Z",
     "iopub.status.idle": "2023-04-08T09:02:50.301588Z",
     "shell.execute_reply": "2023-04-08T09:02:50.300479Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.295665Z"
    },
    "id": "WH5Ca5fvfN1J",
    "outputId": "c48f7813-0ad5-4c8d-854d-1ada68472eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "punct = string.punctuation\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqrObkD7h0mQ"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "Here in the reviews we will find many stop words which do not add any meaning to the review.\n",
    "Also punctuations will be encountered in the review which which will be considered as a seperate token by our model\n",
    "So removing all the stop words and punctuation so that our model can train efficiently\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.303021Z",
     "iopub.status.busy": "2023-04-08T09:02:50.302726Z",
     "iopub.status.idle": "2023-04-08T09:02:50.314370Z",
     "shell.execute_reply": "2023-04-08T09:02:50.313534Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.302991Z"
    },
    "id": "HKxbf5WBfhwg"
   },
   "outputs": [],
   "source": [
    "def dataCleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != '-PRON-':\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in punct and token not in stopwords:\n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here after passing a particular sentence in dataCleaning method we are returned with relevant words which contribute to the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.316204Z",
     "iopub.status.busy": "2023-04-08T09:02:50.315710Z",
     "iopub.status.idle": "2023-04-08T09:02:50.341181Z",
     "shell.execute_reply": "2023-04-08T09:02:50.340329Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.316169Z"
    },
    "id": "yVT_Yym0js_N",
    "outputId": "4dd878b2-34be-400d-abb1-d52fd3c6c32a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'heavy',\n",
       " 'rainfall',\n",
       " 'recommend',\n",
       " 'stay',\n",
       " 'home',\n",
       " 'safe',\n",
       " 'start',\n",
       " 'run']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCleaning(\"Today we are having heavy rainfall, We recommend you to stay at your home and be safe, Do not start running here and there\")\n",
    "# All the useful words are returned, no punctuations no stop words and in the lemmatized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:02:50.344029Z",
     "iopub.status.busy": "2023-04-08T09:02:50.343592Z",
     "iopub.status.idle": "2023-04-08T09:02:50.355146Z",
     "shell.execute_reply": "2023-04-08T09:02:50.354205Z",
     "shell.execute_reply.started": "2023-04-08T09:02:50.343983Z"
    },
    "id": "eTxGmm0_ktFt",
    "outputId": "8be908e7-2dd9-46a4-d8a2-ade433486049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4361,) (485,)\n"
     ]
    }
   ],
   "source": [
    "# Spillting the train and test data\n",
    "X = data['Sentence']\n",
    "y = data['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print(X_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:03.973447Z",
     "iopub.status.busy": "2023-04-08T09:05:03.972922Z",
     "iopub.status.idle": "2023-04-08T09:05:03.979538Z",
     "shell.execute_reply": "2023-04-08T09:05:03.978445Z",
     "shell.execute_reply.started": "2023-04-08T09:05:03.973411Z"
    },
    "id": "xKwAQK5qktD0"
   },
   "outputs": [],
   "source": [
    "# Creating the model and pipeline\n",
    "tfidf = TfidfVectorizer(tokenizer = dataCleaning)\n",
    "svm = RandomForestClassifier()\n",
    "steps = [('tfidf',tfidf),('svm',svm)]\n",
    "pipe = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:08.383028Z",
     "iopub.status.busy": "2023-04-08T09:05:08.382533Z",
     "iopub.status.idle": "2023-04-08T09:05:29.681081Z",
     "shell.execute_reply": "2023-04-08T09:05:29.680293Z",
     "shell.execute_reply.started": "2023-04-08T09:05:08.382993Z"
    },
    "id": "ZlaTICpJktC1",
    "outputId": "f0bfe29b-b61d-4395-f41c-77d6d4ee356b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function dataCleaning at 0x000001DDB2EF6040>)),\n",
       "                ('svm', RandomForestClassifier())])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:29.683423Z",
     "iopub.status.busy": "2023-04-08T09:05:29.683140Z",
     "iopub.status.idle": "2023-04-08T09:05:34.714112Z",
     "shell.execute_reply": "2023-04-08T09:05:34.713091Z",
     "shell.execute_reply.started": "2023-04-08T09:05:29.683396Z"
    },
    "id": "IavuQuDJktAH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0, -1,  1,  0,  0,  0,\n",
       "        0,  0, -1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  1,  0,  0,  0,  0,  0, -1,  0,  0,  1,  0,  1,  0,  0,\n",
       "        0,  1,  0,  1, -1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
       "        0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        1,  0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,\n",
       "        1,  1,  0,  0,  1,  1,  0,  0, -1,  0,  0,  1,  1,  0,  0,  0,  0,\n",
       "        1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
       "        0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0, -1,  0,  0,  0,  0,  0,\n",
       "        0,  0, -1,  0, -1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0, -1,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  0,\n",
       "       -1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1,\n",
       "        0,  0,  1,  0,  1,  1,  0,  1,  0,  0,  0, -1,  1,  1,  1,  0, -1,\n",
       "        1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0, -1,  0,  0,  0,\n",
       "        0,  0,  1,  0,  1,  1,  0,  0,  0,  1,  0,  1, -1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0, -1,  0,  1,  0,  0,  1,\n",
       "        1,  0, -1,  1,  0,  0,  0,  0,  0, -1,  0, -1,  0,  0,  0,  0,  0,\n",
       "        0, -1,  1,  1,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0,  1,  0,  0,\n",
       "        0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,\n",
       "        0,  0,  0,  1,  1,  1, -1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "        1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "        0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  1, -1,\n",
       "        1,  1,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1, -1,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on the test dataset\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463917525773196"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:34.715929Z",
     "iopub.status.busy": "2023-04-08T09:05:34.715622Z",
     "iopub.status.idle": "2023-04-08T09:05:34.729068Z",
     "shell.execute_reply": "2023-04-08T09:05:34.727971Z",
     "shell.execute_reply.started": "2023-04-08T09:05:34.715899Z"
    },
    "id": "2mPpPGkDmCzX",
    "outputId": "32d56cfe-f7f4-4e0d-ae07-c6ab0c12bfde"
   },
   "outputs": [],
   "source": [
    "# # Printing the classification report and the confusion matrix\n",
    "# print(classification_report(y_test,y_pred))\n",
    "# print(\"\\n\\n\")\n",
    "# print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the Random Manual Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here '1' represent that the input is positive sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:34.731087Z",
     "iopub.status.busy": "2023-04-08T09:05:34.730647Z",
     "iopub.status.idle": "2023-04-08T09:05:34.749766Z",
     "shell.execute_reply": "2023-04-08T09:05:34.748900Z",
     "shell.execute_reply.started": "2023-04-08T09:05:34.731043Z"
    },
    "id": "TOcACsrQks9x",
    "outputId": "373e6231-6ae6-4473-ddbf-a4b88cb8386a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on random inputs\n",
    "pipe.predict([\"Wow you are an amazing person\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here '0' represent that input is negative sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T09:05:34.752048Z",
     "iopub.status.busy": "2023-04-08T09:05:34.751756Z",
     "iopub.status.idle": "2023-04-08T09:05:34.768446Z",
     "shell.execute_reply": "2023-04-08T09:05:34.767562Z",
     "shell.execute_reply.started": "2023-04-08T09:05:34.752019Z"
    },
    "id": "Pfo44Ij9m2OY",
    "outputId": "cd64a1e6-6882-4fc9-9f81-c6cf3e861b1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"you suck\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbOH_5vcgO6s"
   },
   "source": [
    "### Footnotes\n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n",
    "\n",
    "https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
